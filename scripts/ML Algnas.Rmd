
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## ML codes

Now are trying to practice the machine learning for irisi data. First we dload the packages:

```{r,warning=FALSE,message=FALSE}
library(table1)
library(here)
library(flextable)
library(tidyverse)
library(readxl)
#install.packages("mice")
library(MASS)
library(ggplot2)
#options("scipen"=100, "digits"=2)
#library(mice)
library(lattice)
library("data.table")
library(car)
library(randomForest)
library(tree)
library(rpart)
library(e1071)
library(caret)
library("ROSE")
library(pROC)
library(ellipse)
library(xgboost)
library(gbm)
library(nnet)
library(NeuralNetTools)
library(ROSE)
#library(mlbench)
```



## iris data set

```{r read data}
raw <- read.csv2(here("data", "Raw data clean.csv" ))

# Select relevant columns and prepare dataset
db <- raw %>% 
  dplyr::select(ID, Point.digging, Area, Digging.depth, Uneven.digging.depth, Seedlings.2023, Big.Small.2023,
         Total.outside.2023, Big.small.2022, Surroundings, Orientation, Djup.cm) %>% 
  mutate(ID = as.factor(ID)) %>% 
  mutate(Point.digging = as.factor(Point.digging)) %>% 
  mutate(Uneven.digging.depth = as.factor(Uneven.digging.depth)) %>% 
  mutate(Surroundings = as.factor(Surroundings)) %>% 
  mutate(Orientation = as.factor(Orientation)) %>% 
  mutate(Djup.cm = as.factor(Djup.cm))
str(db)

```   

## Table 1

```{r table1}
table1(~., data=db )
```

\pagebreak

## train and test

```{r}
###### create a list of 70% of the rows in the original dataset we can use for training
Training_index <- createDataPartition(db$Digging.depth,
                                     p = 0.7,
                                     list = FALSE)
# Training Data
    Training_data <- db[Training_index, ]
        dim(Training_data)
          sapply(Training_data, class)
            head(Training_data)
                levels(Training_data$Digging.depth)

# select 30% of the data for Testing
Testing_data <- db[-Training_index , ]



# summarize the class distribution
percentage <- prop.table(table(Training_data$Digging.depth)) * 100
          cbind(freq = table(Training_data$Digging.depth), percentage = percentage)
          
```
## split input and output

```{r}

x <- Training_data[,c(2:5, 8:ncol(db))]
y <- Training_data[,4]

#####.............................................

# boxplot for each attribute on one image
par(mfrow = c(1,4))
for(i in 3:ncol(db)) {
  boxplot(x[,i], main = names(db)[i])
}
```
\pagebreak

## some plots

```{r}
par(mfrow = c(1,1))
# scatterplot matrix
featurePlot(x = x, y = y, plot = "ellipse")

# box and whisker plots for each attribute
featurePlot(x = x, y = y, plot = "box")

```
```{r}
# density plots for each attribute by class value
scales <- list(x=list(relation = "free"), y=list(relation = "free"))
       featurePlot(x = x, y = y, plot = "density", scales = scales)
   
```

## set the cross-validation and metric

```{r cross val}
# Run algorithms using 10-fold cross Testing
control <-  trainControl(
  method = "cv", number = 10,
        summaryFunction = twoClassSummary,
             classProbs = TRUE # Super important!
  # verboseIter = TRUE
)
metric <- "ROC" # binary outcome varaibles
```

## Logistics regression

```{r logistics reg}
set.seed(7)

# fit logistic regression
fit.glm <- train(Species~., data = Training_data,
               method = "glm",
               metric = metric,
               trControl = control)

```

## LASSO

```{r lasso fit}
#####..........................................
# fit LASSO, Elastic, Net
fit.glmnet <- train(Species~., data = Training_data,
                  method = "glmnet",
                  metric = metric,
                  trControl = control)
```


```{r knn fit}

fit.knn <- train(Species~.,
                 data = Training_data, 
                 method = "knn",
                 metric = metric,
                 trControl = control)
```

## DT

```{r dt fit}
fit.DT <- train(Species~.,
                 data = Training_data, 
                 method = "rpart",
                 metric = metric,
                 trControl = control)

#plot(fit.DT$finalModel, uniform = TRUE,
 #    main="Classification Tree")
       #   text(fit.DT$finalModel,
            #   all = TRUE, 
               #col = 2,
               #cex = .8)

# DT Plots
# suppressMessages(library(rattle))
#   fancyRpartPlot(fit.DT$finalModel)

library(rpart.plot)
  rpart.plot(fit.DT$finalModel)
```
## RF

```{r rf fit}
# fit random forest

fit.rf <- train(Species~.,
                 data = Training_data, 
                 method = "rf",
                 metric = metric,
                 trControl = control)

```

```{r gbm}
library(xgboost)
fit.xgboost <- train(Species~.,
                data = Training_data, 
                method = "gbm", #
                metric = metric,verbose = FALSE,
                trControl = control)
```

## Varible importance

```{r var imp plot}
plot(varImp(fit.xgboost))
```


## SVM

```{r svm}
# linear SVM
fit.svm <- train(Species~.,
                data = Training_data, 
                method = "svmLinear",
                metric = metric,
                trControl = control)

###############################################
# None-linear SVM
library(kernlab)
fit.svm_nonlinear <- train(Species~.,
                 data = Training_data, 
                 method = "svmRadial",
                 metric = metric,
                 trControl = control)

```


```{r fit ann1}

fit.ann<- train(Species~.,
                 data = Training_data, 
                 method = "nnet",
                 metric = metric,
                 preProcess = "range", # apply min-max normalization
                 trControl = control
                 #, tuneGrid = grids # search over the created grid
                 ,trace = FALSE)

```

```{r plot for ann1}
plot(fit.ann )
```

```{r}
library(NeuralNetTools)

plotnet(mod_in = fit.ann$finalModel, # nnet object
        pos_col = "darkgreen", # positive weights are shown in green
        neg_col = "darkred", # negative weights are shown in red
        bias = FALSE, # do not plot bias
        circle_cex = 4, # reduce circle size (default is 5)
        cex_val = 0.6) # reduce text label size (default is 1)
```

## ANN

We can use the **train()** function from the caret package to tune our hyperparameters. Here, we will use the **nnet** package (method = "nnet"). We can tune the size and decay hyperparameters.

* size: number of nodes in the hidden layer Note: There can only be one hidden layer using nnet
* decay: weight decay. regularization parameter to avoid overfitting, which adds a penalty for complexity.

First, we set up the grid using the expand.grid() function. We will consider hidden node sizes (size) of 1, 3, 5 and 7 and decay values ranging from 0 to 0.1 in 0.01 increments.

```{r}
grids <-  expand.grid(size = seq(from = 1, 
                                 to = 7, 
                                 by = 2),
                      decay = seq(from = 0,
                                  to = 0.1,
                                  by = 0.01))
```


```{r}
ctrl <- trainControl(method = "cv",
                     number = 10, # 5 folds
                     repeats = 10, # 3 repeats
                     search = "grid",
                     classProbs = TRUE) # grid search
```


```{r fit ann2}

fit.ann2 <- train(Species~.,
                 data = Training_data, 
                 method = "nnet",
                 metric = "ROC",
                 preProcess = "range", # apply min-max normalization
                 trControl = ctrl
                 , tuneGrid = grids, # search over the created grid
                 trace = FALSE)

```



```{r plot for ann}
plot(fit.ann2 )
```

```{r}
library(NeuralNetTools)

plotnet(mod_in = fit.ann2$finalModel, # nnet object
        pos_col = "darkgreen", # positive weights are shown in green
        neg_col = "darkred", # negative weights are shown in red
        bias = FALSE, # do not plot bias
        circle_cex = 4, # reduce circle size (default is 5)
        cex_val = 0.6) # reduce text label size (default is 1)
```



## summary  of results in training data



```{r}
#.................. summarize accuracy of models ........................
#
results <- resamples(list(logistic_regression = fit.glm,
                          Elastic_net = fit.glmnet,
                         # lda = fit.lda,
                         # qda = fit.qda, 
                          knn = fit.knn,
                          DT = fit.DT,
                          RF = fit.rf,
                          XGboost = fit.xgboost,
                          SVM_linear1 = fit.svm,
                          SVM_nonlinear = fit.svm_nonlinear,
                         ANN=fit.ann 
                          
                          ))
summary(results)

##########################
# compare accuracy of models
dotplot(results)
```
## ROC curve

```{r}
#..................... estimate skill of LDA on the Testing dataset
predictions_svm <- predict(fit.svm_nonlinear, Testing_data )
Testing_data $Species
table(predictions_svm , Testing_data $Species)
confusionMatrix(predictions_svm , Testing_data$Species)

############
predictions_svm_test2 <- predict(fit.svm_nonlinear, Testing_data ,type = "prob")
     predictions_svm2 <- predict(fit.svm_nonlinear, Testing_data)
predictions_svm2[1:10]
```
```{r}
par(pty = "s")
# creat confusion matrix for comparison

conmatrix_svm2 <- confusionMatrix(as.factor(predictions_svm2 ),
                                  as.factor(Testing_data$Species))

      outcome = ifelse(Testing_data $Species == "NO", 0, 1)
      
      ##### create ROC
roc_svm_test2 = roc(outcome, (predictions_svm_test2$Yes),
                    plot=TRUE,
                    ci=TRUE,
                   legacy.axes=TRUE, percent=TRUE,
                   xlab="False Positive Percentage",
                   ylab="True Postive Percentage", 
                   col=1,
                   lwd=4,
                   print.auc=TRUE,
                   print.auc.y=60)



```



## Random Over sampling

Functions to deal with binary classification problems in the presence of imbalanced classes. Synthetic balanced samples are generated according to ROSE (Menardi and Torelli, 2014). Functions that implement more traditional remedies to the class imbalance are also provided, as well as different metrics to evaluate a learner accuracy. These are estimated by holdout, bootrstrap or cross-validation methods.

### Example

```{r }
data(hacide)

# check imbalance
table(hacide.train$cls)

```

```{r}
# train logistic regression on imbalanced data
log.reg.imb <- glm(cls ~ ., data=hacide.train, family=binomial)
# use the trained model to predict test data
pred.log.reg.imb <- predict(log.reg.imb, newdata=hacide.test,
                            type="response")
```


```{r}
# generate new balanced data by ROSE
hacide.rose <- ROSE(cls ~ ., data=hacide.train, seed=123)$data

# check (im)balance of new data
table(hacide.rose$cls)
```

```{r}
# train logistic regression on balanced data
log.reg.bal <- glm(cls ~ ., data=hacide.rose, family=binomial)
```

```{r}
# use the trained model to predict test data
pred.log.reg.bal <- predict(log.reg.bal, newdata=hacide.test,
                            type="response")
```

## compare balanced and imbalanced

```{r}
# check accuracy of the two learners by measuring auc
roc.curve(hacide.test$cls, pred.log.reg.imb)
roc.curve(hacide.test$cls, pred.log.reg.bal, add.roc=TRUE, col=2)
```

## Roc Eval
```{r evalboot}
# determine bootstrap distribution of the AUC of logit models
# trained on ROSE balanced samples
# B has been reduced from 100 to 10 for time saving solely
boot.auc.bal <- ROSE.eval(cls ~ ., data=hacide.train, learner= glm,
                          method.assess = "BOOT",
                          control.learner=list(family=binomial),
                          trace=TRUE, B=10)

summary(boot.auc.bal)
```

